{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMcvCCv+972YSHzeMex/vFO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eschwarzbeckf/taxi/blob/main/taxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Setup"
      ],
      "metadata": {
        "id": "RAWqLlA0TbL5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqhTvJ4-Mu2U",
        "outputId": "fcc3f8bb-1de2-4df6-a711-1157e0de398f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'taxi'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 15 (delta 2), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 4.11 KiB | 4.11 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/taxi\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
            "Collecting pyvirtualdisplay (from -r requirements.txt (line 2))\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium->-r requirements.txt (line 1)) (0.0.4)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/eschwarzbeckf/taxi.git\n",
        "%cd taxi\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Imports"
      ],
      "metadata": {
        "id": "ok9utJMhTq-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import deque\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import io, glob, base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ],
      "metadata": {
        "id": "xjA4Lha1M1J1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Code"
      ],
      "metadata": {
        "id": "-ipgwlJFrJCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Env Setup"
      ],
      "metadata": {
        "id": "qcJ5pegqrLw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "6w2d_-Erz8uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video(name):\n",
        "  mp4list = glob.glob(f'./{name}.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env, name):\n",
        "  video = VideoRecorder(env, f'./{name}.mp4')\n",
        "  return env, video"
      ],
      "metadata": {
        "id": "wSrpPXwAONVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(q_values, start, end, decay, step):\n",
        "  epsilon = (\n",
        "      end + (start-end) * math.exp(-step / decay)\n",
        "  )\n",
        "  sample = random.random()\n",
        "  if sample < epsilon:\n",
        "    return random.choice(range(len(q_values)))\n",
        "\n",
        "  return torch.argmax(q_values).item()"
      ],
      "metadata": {
        "id": "CAiqo8xXxGam"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.embedding = nn.Embedding(state_size, 64)\n",
        "    self.fc1 = nn.Linear(64, 64)\n",
        "    self.fc2 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.embedding(state)\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    return self.fc2(x)"
      ],
      "metadata": {
        "id": "1UNrW9lrNlVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "    experience_tuple = (state, action, reward, next_state, done)\n",
        "    self.memory.append(experience_tuple)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    batch = random.sample(self.memory, batch_size)\n",
        "    states, actions, rewards, next_states, dones = (zip(*batch))\n",
        "\n",
        "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
        "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n",
        "    next_states_tensor = torch.tensor(next_states, dtype=torch.float32)\n",
        "    dones_tensor = torch.tensor(dones, dtype=torch.float32)\n",
        "    actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
        "    return states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor"
      ],
      "metadata": {
        "id": "2sI6-JLd0DWZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_target_network(target_network, online_network, tau):\n",
        "  target_net_state_dict = target_network.state_dict()\n",
        "  online_net_state_dict = online_network.state_dict()\n",
        "  for key in target_net_state_dict.keys():\n",
        "    target_net_state_dict[key] = (\n",
        "        online_net_state_dict[key] * tau + target_net_state_dict[key] * (1-tau)\n",
        "    )\n",
        "    target_network.load_state_dict(target_net_state_dict)"
      ],
      "metadata": {
        "id": "y3Z7QgZqBLHf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3', render_mode='rgb_array')\n",
        "env = gym.wrappers.RecordVideo(env, video_folder=\"./video_directory\")\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "mw2n7oJOZGIZ",
        "outputId": "6f2f6a5f-8812-438c-94bf-02e1d4efeda0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x784b9dcec690>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "online_network = QNetwork(state_size, action_size)\n",
        "target_network = QNetwork(state_size, action_size)\n",
        "target_network.load_state_dict(online_network.state_dict())\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "optimizer = torch.optim.Adam(online_network.parameters(), lr=0.001)\n",
        "batch_size = 64\n",
        "gamma = 0.95\n",
        "tau = 0.005\n",
        "total_steps = 0\n",
        "\n",
        "for episode in range(20000):\n",
        "  state, info = env.reset()\n",
        "  done = False\n",
        "  step = 0\n",
        "  episode_reward = 0\n",
        "  while not done:\n",
        "    step += 1\n",
        "    total_steps += 1\n",
        "    state_tensor = torch.tensor([state], dtype=torch.long)\n",
        "    q_values = online_network(state_tensor)\n",
        "    action = select_action(q_values, 0.9, 0.05, 1000, step)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    replay_buffer.push(state,action,reward,next_state,done)\n",
        "    if len(replay_buffer) >= batch_size:\n",
        "      states, actions,rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "      states = states.long()\n",
        "      actions = actions.long()\n",
        "      next_states = next_states.long()\n",
        "\n",
        "      q_values = online_network(states).gather(1, actions).squeeze(1)\n",
        "      with torch.no_grad():\n",
        "        next_q_values = (\n",
        "          target_network(next_states).amax(1)\n",
        "        )\n",
        "        target_q_values = (\n",
        "            rewards + gamma * next_q_values * (1 - dones)\n",
        "        )\n",
        "      loss  = torch.nn.functional.mse_loss(q_values, target_q_values)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      update_target_network(target_network, online_network, tau)\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "V6rVNX8q4PdX"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}