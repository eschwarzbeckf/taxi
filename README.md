# Gymnasium Taxi ðŸš•

## Overview
- Notebook has two algorithms DDNQ with Prioritization and traditional QTable
- This is for a school project and learning, nothing fancy
- It also creates videos to see the performance of the agent

## Results
- We can see that DDNQ w Prio learns faster (less episodes) than QTable
- QTable aprox. requires 3500 episodes, rather than the DDQN requires 1500 to reach the same reward average
- On the other hand, QTable is quicker overall, since processing each iteration requires less resources (or CPU power)
  ![image](https://github.com/user-attachments/assets/3bbefd88-6afc-402c-99e9-8f63669e527f)
